---
layout: post
title: "卷积神经网络（二）：使用keras + tensorflow构造CNN进行训练"
date: 2020-10-11
author: Tianlh
header-img: "img/head-bg1.jpg"
tags:
  - 卷积神经网络
  - CNN
  - 深度学习
  - keras
  - tensorflow
---

#### **相关库**

##### **scikit-learn**
- scikit-learn是一个通用的机器学习库，提供很多传统机器学习的实现，并且高度封装，用户可以很便捷的使用。另外，由于传统的机器学习方法注重特征的提取，因此scikit-learn提供了强大的特征工程相关函数。
- <https://scikit-learn.org/stable/tutorial/machine_learning_map/>
- ![image]({{ "/img/post/fhsksjkgb3r9083402rhrk0erewu9e89fdsjk.png" }})

##### **tensorflow**
- tensorflow是一个深度学习库，它的自由度很高。并且由于深度学习算法本身就是一个特征学习的过程，因此不需要人工进行特征提取，tensorflow也就没有提供太多专门提取特征的函数。

##### **theano**
- theano与tensorflow类似，也是一个深度学习库。

##### **keras**
- keras是基于python的神经网络库，实现了对tensorflow和theano的封装，借助keras可以方便的搭建网络模型。

#### **环境配置**
- 配置的时候注意python版本和tensorflow版本需要匹配，我用的python 3.6-64搭配tensorflow 2.x。
- tensorflow 2.x 和 1.x 整个架构都不一样了，注意选择版本。tensorflow 1.x 使用静态图模式，性能比较占优势，但是debug不方便。tensorflow 2.x 使用动态图模式，这种形式和 python 执行比较接近，语句按顺序执行，也比较好上手。
- CPU版本使用pip一键安装（pip install tensorflow）。
- 不能直接连外网就比较麻烦，只能手动安装，去https://pypi.org/上自己下载tensorflow和依赖库的安装包，一个个手动安装。<br/>
安装包一般是.whl或者压缩包。如果是.whl格式，使用```pip install xxx.whl```命令安装。<br/>
如果是压缩包，解压后会有setup.py文件，执行命令安装<br/>
```python setup.py build```<br/>
```python setup.py install```
- GPU版本安装还需要额外配置环境

#### **模型训练**

##### **数据预处理**
- 数据采集打标签，采集的数据可以以.csv格式保存。
- 加载数据，划分训练集和测试集<br/>
```python
import pandas as pd
import tensorflow as tf 
data = pd.read_csv(path_name)
```
// 输入数据为1 * w的数据，一共M组数据， M * w <br/>
// 最后一列为label
```python
col = data.shape[1] - 1
dataTrain, dataTest, labelTrain, labelTest = train_test_split(data[data.columns[0:col]].values, data['label'].values, test_size = 0.3, random_state = 0)
```
// 转化格式<br/>
// 以conv2D为例，默认input_shape为channels_last，即数据以[batch
, height, width, channels]的排列顺序
```python
xTrain = tf.reshape(dataTrain, [dataTrain.shape[0], 1, ])
```
label一般需要处理一下，比如转换成使用one-hot编码的形式

##### **利用keras构建模型**
```python
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Reshape, Droput
def CNN()
    model = mode.sequential()
    model.add(Conv2D(32, (1, 3), activation = 'relu', padding = 'same'))
    model.add(Conv2D(64, (1, 3), activation = 'relu', padding = 'same'))
    model.add(MaxPooling2D((1,2)))
    model.add(Flatten())
    model.add(Dropout(0.4))
    model.add(Dense(32, activation = 'softmax'))
    return model

model = CNN()
model.summary()
```
- 添加网络层前，可以加些预处理，如reshape操作
- ###### Conv2D：卷积层
  - 卷积层的核心处理就是卷积操作，卷积操作的处理过程如下图，输入数据以二维数据为例，拿一个3 * 3的kernel去扫输入数据，kernel中心依次与输入数据的每个数据重合，比如图中kernel中心位于输入数据[1,1]的位置，将当前kernel窗口每个数据与它覆盖输入数据相乘，然后再把9个相乘后的结果相加，得到的就是[1,1]位置卷积后的结果。
  ![image]({{ "/img/post/hlassalfhsal2jjehlhh43lh3l4l3243l4jhk.png" }})
  - 卷积层填充（padding）方式为same，这指明卷积操作对边缘的处理方式，当kernel中心位于[0,0]位置时，一部分kernel位于输入数据外，此时如果padding方式选valid，那么该点的结果会被丢弃，8 * 6的输入数据处理完后变成6 * 4的数据。如果选same的方式，那么当kernelc窗口超出输入数据范围时，会按一定规则补上超出范围的数据，使得处理完的数据大小保持不变。
    ![image]({{ "/img/post/hhio4uoi1299cz0xc8z0zc8xzv9d78dv88v8c.png" }})
  - 回到本文的例子中，输入样本为1 * w的一维数据
  - 第一层卷积层depth为32，kernel_size为1 * 3， 激活函数为relu，这样input_shape = [b, 1, w, 1]，output_shape = [b, 1, w, 32]，kernel_shape = [1, 3, 1, 32]
  - 第二层卷积层，depth为64，因为上层卷积层的depth为32，所以本层输入数据的depth为32，这样input_shape = [b, 1, w, 32]，output_shape = [b, 1, w, 64]，kernel_shape = [1, 3, 32, 64]
  - 卷积层处理原理如下，以第二层卷积层为例，先不考虑性能。如果输入样本为多维数据，kernel也会是多维的，实现会更复杂一些，不过原理一样：
```c++
    for (auto featureIdx : w) {
        for (auto outputChannel : 64) {
            auto sum = 0;
            for (auto inputChannel : 32) {
                // 单个卷积核处理, 1 * 3的卷积核
                auto convRet = 0;
                for (auto kernelIdx = -1; kernelIdx < 2; kernelIdx++) {
                    convRet += inputData[(kernelIdx + featureIdx) * 32 + inputChannel] * 
                        kernel[(kernelIdx + 1) * 64 * 32 + inputChannel * 32 + outputChannel];
                }
                sum += convRet;
            }
            // 输出数据的排列顺序为[1, w, 64]
            outputData[featureIdx * 64 + outputChannel] = sum;
        }
    }
```
- ###### 池化层：MaxPooling2D
  - pool_size为1 * 2，另外还可以设置strides，也就是池化窗口处理的步长，此处步长为1 * 2。
  - MaxPooling的原理是取池化窗口里的最大值，池化处理后数据量变小，outpue_shape = [b, 1, w / 2, 64]，这样可以减少计算量，也能避免过拟合。
- ###### Flatten
  - 把数据拉平，变成1维数据。
- ###### Dropout
  - 丢弃40%的输入数据，防止过拟合。
- ###### 全连接层：Dense
  - Dense的原理为outputData = Activation(inputData * weight + bias)
  - 当输入数据为[b, 1, w * 32]，weight_shape为 = [1, w *32, 32]，bias_shape = [1, 1, 32]，output_shape = [b, 1, 32]

##### **训练**
// 转换数据格式，与model一致
// labelTrain2采用ont-hot编码，比如三种标签，标签1转化为[0,0,1]，标签2转化为[0,1,0]，标签3转化为[1,0,0]
```python
dTrain = tf.reshape(dataTrain, [dataTrian.shape[0], 1, dataTrain.shape[1], 1])
lTrain = tf.reshape(labelTrian2, [-1, 3])
```
// 设置训练器，损失函数，准确率评估方法
```python
model.compile(optimizer = 优化器, loss = 损失函数, metrics = ["准确率评估方法”])
```
// 训练，迭代75次，每次使用512个样本
```python
fit = model.fit(dTrain, lTrain, batch_size = 512, epochs = 75)
```
// 测试训练结果
```python
loss_acc = model.evaluate(xTest, yTest)
```

##### **模型保存**
```python
model.save("XXX.h5")
```
默认的模型格式为.h5，为了工程化，我对模型做了一些简化，导出成json，方便后续准换成C数据结构
