---
layout: post
title: "卷积神经网络（一）"
date: 2020-10-11
author: Tianlh
header-img: "img/head-bg1.jpg"
tags:
  - 卷积神经网络
  - CNN
  - 深度学习
---

##### **传统神经网络**
![image]({{ "/img/post/ab502e8bdbf2a5da5b3ff4500dbac9f8.png" }})
- 图例是一个单隐层神经网络，包含i个输入层神经元、h个隐层神经元和o个输出层神经元，其参数包含i * h个输入层到隐层的权值，h个隐层神经元的阈值，h * o个隐层到输出层神经元的权值，o个输出层神经元的阈值。

##### **卷积神经网络**
- 卷积神经网络是一种多层神经网络，通过”权共享”策略，在保持多层神经网络强大的学习能力基础上，节省训练开销。
![image]({{ "/img/post/4fec4c88fbf6a6dff0ab76c43b635f1e.png" }})
- 基本的卷积神经网络包括输入层、卷积层、池化层、全连接层、输出层。

##### **卷积层**
- 卷积层包含卷积滤波器和激活函数（或者单独作为激励层ReLU layer处理）
- 卷积层的每一个神经元为一个k * k的核，每一组神经元（即图中的每个平面）使用相同连接区权（即”权共享“）。神经元组的个数为深度（depth）。如第一个卷积层深度为6，每组神经元提取一个特征映射。
- 权共享：输入为10 * 10的图像，假如传统神经网络的隐层使用100个神经元，那么不考虑bias，权值参数需要有 10 * 10 * 100个。如果每个神经元负责5 * 5的区域，那么需要 5 * 5 * 100个参数，如果每个神经元使用同样的参数，那么最终只需要 5 * 5个权值参数。一个卷积核提取出的一种特征称为feature map，要提取多种特征就增加卷积核数量。
- CNN还具有稀疏连接特性：卷积操作是一个局部敏感的操作，多层卷积层呈现如下关系：
![image]({{ "/img/post/4fedasf8fbf6a6dff0ab76c43b635f1e.png" }})
- 卷积层的处理过程就是对本层的输入做卷积操作，再加上dias。

##### **激励层**
- 卷积层的输出通过激活函数进行映射。
- 激活函数包括 Sigmoid、ReLU、Tanh、Softmax等。之前项目做一些多分类的训练，Softmax会比较好用。

##### **池化层**
- 池化层对输入数据做重采样，经过这一层，数据长宽缩小，但是深度不变。
- 通过池化层可以对数据降维，一定程度上防止过拟合。

##### **全连接层**
- 在全连接层之前通常还有一个Flatten操作，把输入数据展平，转化为一个一维数组。
-  全连接层的处理和传统神经网络一致：经过加权处理再通过激活函数映射。


