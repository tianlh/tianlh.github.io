---
layout: post
title: "PCA主成分分析"
date: 2020-11-21
author: Tianlh
header-img: "img/head-bg1.jpg"
tags:
  - 数据降维
  - PCA
  - 主成分分析
  - OpenCV
---

<small>*——它亦会毁灭我又重塑我*<small>

主成分分析（Principal Component Analysis，简称PCA）是种很经典的降维手段。在各种机器学习相关的方法中也很常见。

#### **降维**
样本数据处理过程中往往遇到数据纬度过高的问题，此时对数据处理造成极大的困难。解决这种数据灾难的一种有效手段是降维。
PCA就是这样一种手段，找到一个合适的低维特征空间，通过将高维数据投影到低维空间，使得在最大程度保留样本特征的前提下，减少数据量。什么事合适的低维特征空间，它应该具有以下特征：
- 最近重构性：样本数据投影后离投影的超平面距离都足够近
- 最大可分性：样本数据投影后能尽可能分开
如下图，将一组二维数据投影到红色坐标轴上，因为以上特征，投影后的点有足够区分度，因此这组二维数据就可以用新坐标轴上的数据来表示，变成一维数组。
![image]({{ "/img/post/h121h43k4hk1h4twb1j5h34l51h3l514h51l3.png" }})

#### **协方差矩阵**
协方差用于衡量两个变量的总体误差，当两个变量相同时，就是方差。协方差计算公式如下：\\[Cov(X, Y) = E[(X - E[X])(Y - E[Y])] = E[XY] - E[X]E[Y]\\]
协方差矩阵就是矩阵中各个向量两两计算协方差得到的结果组成的矩阵。如矩阵$X = (X_{1}, X{2}, ..., X_{n})^{T}$，它的协方差矩阵为：
\\[
C =  \begin{bmatrix}
Cov(X_{1}, X_{1}) & Cov(X_{1}, X_{2}) & \cdots & Cov(X_{1}, X_{n}) \\\\\
Cov(X_{2}, X_{1}) & Cov(X_{2}, X_{2}) & \cdots & Cov(X_{2}, X_{n}) \\\\ 
\vdots & \vdots & \ddots & \vdots \\\\ 
Cov(X_{n}, X_{1}) & Cov(X_{n}, X_{2}) & \cdots & Cov(X_{n}, X_{n}) \\\\
\end{bmatrix}
\\]

#### **特征分解**
如果一个值$\lambda$和一组向量$\upsilon$满足\\[A\upsilon = \lambda\upsilon\\]那么$\upsilon$为矩阵A的特征向量，$\lambda$为特征值。矩阵的一组特征向量为正交向量。
特征值分解是将N * N的方阵A分解成：\\[A = Q\Sigma Q^{-1}\\]其中Q为A的特征向量组成的矩阵，也是N * N的方阵。$\Sigma$是一个对角矩阵，它对角线上的元素为特征向量对应的特征值。
特征分解只适用于方阵。

#### **SVD奇异值分解**
奇异值分解适用于任意矩阵。如果A是一个N * M的矩阵，满足\\[A = U\Sigma V^{T}\\]时，U是一个N * N的方阵，U中的向量正交，为左奇异向量，$\Sigma$对角线上的值为对应奇异向量的奇异值，V是一个M * M的方阵，V中的向量正交，为右奇艺向量。
奇异值分解过程：
- 对$AA^{T}$求特征分解，得到一组特征向量N * N，这既是左奇异向量U
- 对$A^{T}A$求特征分解，得到一组特征向量M * M，为右奇艺向量V
- 将U，V带入$A = U\Sigma V^{T}$求出奇异值

#### **PCA处理过程**
PCA处理中寻找低维空间的过程可以理解为逐一选取方差最大的方向。假设样本集$D = \{x_{1}, x_{2}, ..., x_{m}\}$；低维空间的维数为$d‘$，样本的协方差矩阵为  $\sum_{i}x_{i}x_{i}^{T}$，对其做特征值分解，选择最大的特征值$\lambda_{1} $对应的特征向量$w_{1}$，再对$\sum_{i}x_{i}x_{i}^{T} - \lambda_{1}w_{1}w_{1}^{T}$ 进行特征值分解，继续选择最大的特征值$w_{2}$对应的特征向量$w_{2}$……这个过程等价于直接对协方差矩阵进行特征值分解，再选取前$d‘$个特征值对应的特征向量，构成低维空间$W = \{w_{1}, w_{2}, ..., w_{m}\}$。
上述过程可以由最近重构性或最大可分性通过数学推导得到，推导过程网上很多，感兴趣可以看看。
总结一下，PCA算法流程如下：
- 对样本进行中心化：$x_{i} - \tfrac{1}{m}\sum_{i = 1}^{m}x_{i}$
- 计算样本协方差矩阵$XX^{T}$
- 对协方差矩阵$XX^{T}$做特征值分解，由于特征值分解只适用于方阵，所以实际操作时通常用奇异值分解代替
- 取特征值最大的$d'$个特征向量$w_{1}, w_{2},...w_{d'}$，他们组成最终的低维投影矩阵$W$。
- 其中$d'$的数量可以按经验值指定。也可以设定一个阈值$t$，按公式$\tfrac{\sum_{i = 1}{d'}\lambda_{i}}{\sum_{i = 1}{d}\lambda_{i}} \geqslant t$计算得到

显然，PCA的处理过程中将特征值最小的$d - d'$个特征向量丢弃了，这不可避免会造成信息减少。丢弃的特征向量过多会导致有效信息缺失，而丢弃的特征向量过少又无法降维效果有限，这一权衡过程其实就是设定阈值$t$的过程。选择合适的阈值可以在保留样本主要信息的基础上达到很好的降维效果，同时特征值很小的特征向量往往是由于噪声的影响产生的，丢弃它们还能在一定程度上滤噪。

#### **PCA的OpenCV实现**
直接截取了之前项目的一部分代码，代码将输入向量vector投影到低维空间（找到主成分），在低维空间上对数据进行缩放，再重建回原来的高维空间，起到加强主要特征的作用。
它主要的函数是：
- cvCalcPCA，其中cvMatVector为输入数据，cvMatAvgVector为平均向量（用于去中心化），cvMatEigenValue为得到的特征值，cvMatEigenVector为得到的特征向量。
- cvProjectPCA投影函数，cvBackProjectPCA重建原始数据
```C++
Mat_<float> avgVector(1, 2);
Mat_<float> eigenValue(1, 2);
Mat_<float> eigenVector(2, 2);
CvMat cvMatVector = vector;
CvMat cvMatAvgVector = avgVector;
CvMat cvMatEigenValue = eigenValue;
CvMat cvMatEigenVector = eigenVector;
cvCalcPCA(&cvMatVector, &cvMatAvgVector, &cvMatEigenValue, &cvMatEigenVector, CV_PCA_DATA_AS_ROW); 
eigenValue(0,0) = eigenValue(0,0)/(eigenValue(0,0)+eigenValue(0,1));
eigenValue(0,1) = eigenValue(0,1)/(eigenValue(0,0)+eigenValue(0,1));
eigenVector(0, 0) = eigenVector(0, 0) * exaggerate*eigenValue(0,0);
eigenVector(0, 1) = eigenVector(0, 1) * exaggerate*eigenValue(0,0);
eigenVector(1, 0) = eigenVector(1, 0) * exaggerate*eigenValue(0,1);
eigenVector(1, 1) = eigenVector(1, 1) * exaggerate*eigenValue(0,1);
cvProjectPCA(&cvMatVector, &cvMatAvgVector, &cvMatEigenVector, &cvMatVector);
cvBackProjectPCA(&cvMatVector, &cvMatAvgVector, &cvMatEigenVector, &cvMatVector);
```
